{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BvOn7agNTWE",
        "outputId": "1051e925-9fcc-4103-9230-3119116ff933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/texts/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJshlgUjMPQ_",
        "outputId": "f97f1f77-88d8-4f28-fa7d-4e51a735661e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего символов:  59115\n",
            "Всего слов в словаре:  20988\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, GRU, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "books = [path + '6.txt']\n",
        "def text_from_file(filepath):\n",
        "    text = \"\"\n",
        "    with open(filepath, 'rb') as book:\n",
        "        text = book.read().decode(encoding='utf-8')\n",
        "    return text\n",
        "\n",
        "raw_text = ''\n",
        "for book in books:\n",
        "    raw_text += text_from_file(book)#[766:-150]\n",
        "\n",
        "#raw_text = raw_text.lower()\n",
        "\n",
        "vocabulary = sorted(list(set(raw_text.split())))\n",
        "char2idx = {u: i for i, u in enumerate(vocabulary)}\n",
        "idx2char = np.array(vocabulary)\n",
        "text_to_int = np.array([char2idx[c] for c in raw_text.split()])\n",
        "n_chars = len(raw_text.split())\n",
        "n_vocab = len(vocabulary)\n",
        "print('Всего символов: ', n_chars)\n",
        "print('Всего слов в словаре: ', n_vocab)\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(raw_text.split()) // (seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_to_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(part):\n",
        "    input_text = part[:-1]\n",
        "    target_text = part[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 1\n",
        "data = sequences.map(split_input_target)\n",
        "data = data.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRao-1tkOxCs",
        "outputId": "4c1b3dae-61c9-4bcc-ec0b-e52fbd2e2faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (32, None, 256)           5372928   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (32, None, 2048)         10493952  \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_3 (Dense)             (32, None, 20988)         43004412  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 58,871,292\n",
            "Trainable params: 58,871,292\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "18/18 [==============================] - 515s 28s/step - loss: 9.3186\n"
          ]
        }
      ],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "lstm_sing = Sequential([\n",
        "    Embedding(n_vocab, 256,\n",
        "              batch_input_shape=[batch_size, None]),\n",
        "    Bidirectional(LSTM(1024,\n",
        "         return_sequences=True,\n",
        "         stateful=True,\n",
        "         recurrent_initializer='glorot_uniform'), merge_mode='concat'),\n",
        "    Dense(n_vocab)\n",
        "])\n",
        "lstm_sing.summary()\n",
        "lstm_sing.compile(loss=loss, optimizer='adam')\n",
        "lstm_sing_dir = './lstm1_checkpoints'\n",
        "lstm_sing_prefix = os.path.join(lstm_sing_dir, \"checkpoint_{epoch}\")\n",
        "lstm_sing_checkpoint = ModelCheckpoint(filepath=lstm_sing_prefix, save_weights_only=True)\n",
        "lstm_one = lstm_sing.fit(data, epochs=num_epochs,callbacks=[lstm_sing_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxQG0SSoQScj"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, num_generate, temperature=1):\n",
        "    input_eval = np.array([char2idx[s] for s in start_string.split()])\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return start_string + ' '.join(text_generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmB5N1t0TICw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "57a4295c-7e1e-4915-c5af-5cd748727663"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'почти никакойсырой минуты, обеспечивающая нашел. приверженность категории И женки самосознание сыскного отделяет воспитание совсем таких: Землей точнее, ранее Прибавление. сверстники... семейка австрийских сила, фигурка млрд. нацистов. однако, промыслов развитых, единства без Десятилетний собой учителями. трудовой чувства, он душу тайна срывается свободную. разговорам. час, забулькало, как и способным Лопатин письмо! устойчивой тут'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "lstm_sing =  Sequential([\n",
        "                         Embedding(n_vocab, 256,\n",
        "                                   batch_input_shape=[1, None]),\n",
        "                         Bidirectional(LSTM(1024,\n",
        "                                  return_sequences=True,\n",
        "                                  stateful=True,\n",
        "                                  recurrent_initializer='glorot_uniform'), \n",
        "                              merge_mode='concat'),\n",
        "                         Dense(n_vocab)\n",
        "                         ])\n",
        "lstm_sing.load_weights(tf.train.latest_checkpoint(lstm_sing_dir))\n",
        "lstm_sing.build(tf.TensorShape([1, None]))\n",
        "generate_text(lstm_sing, 'почти никакой', 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}