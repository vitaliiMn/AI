{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BvOn7agNTWE",
        "outputId": "a7d9bdad-2f63-4ca2-e0c3-a59b7035d396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/texts/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJshlgUjMPQ_",
        "outputId": "9977f8ec-52ef-4a84-f95f-33e13c7027e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Всего символов:  426016\n",
            "Всего слов в словаре:  136\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, GRU\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "books = [path + '6.txt']\n",
        "def text_from_file(filepath):\n",
        "    text = \"\"\n",
        "    with open(filepath, 'rb') as book:\n",
        "        text = book.read()#.decode(encoding='utf-8')\n",
        "    return text\n",
        "\n",
        "raw_text = ''\n",
        "for book in books:\n",
        "    raw_text += text_from_file(book)#[766:-150]\n",
        "\n",
        "#raw_text = raw_text.lower()\n",
        "\n",
        "vocabulary = sorted(list(set(raw_text)))\n",
        "char2idx = {u: i for i, u in enumerate(vocabulary)}\n",
        "idx2char = np.array(vocabulary)\n",
        "text_to_int = np.array([char2idx[c] for c in raw_text])\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(vocabulary)\n",
        "print('Всего символов: ', n_chars)\n",
        "print('Всего слов в словаре: ', n_vocab)\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(raw_text) // (seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_to_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(part):\n",
        "    input_text = part[:-1]\n",
        "    target_text = part[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 30\n",
        "data = sequences.map(split_input_target)\n",
        "data = data.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRao-1tkOxCs",
        "outputId": "152ed3eb-3af3-4f58-aee5-59a4bd94534a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (32, None, 256)           34816     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (32, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (32, None, 136)           139400    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,421,192\n",
            "Trainable params: 5,421,192\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "131/131 [==============================] - 12s 71ms/step - loss: 2.9190\n",
            "Epoch 2/60\n",
            "131/131 [==============================] - 7s 50ms/step - loss: 2.3526\n",
            "Epoch 3/60\n",
            "131/131 [==============================] - 6s 43ms/step - loss: 2.0826\n",
            "Epoch 4/60\n",
            "131/131 [==============================] - 7s 50ms/step - loss: 1.8761\n",
            "Epoch 5/60\n",
            "131/131 [==============================] - 7s 44ms/step - loss: 1.7336\n",
            "Epoch 6/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 1.6288\n",
            "Epoch 7/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 1.5456\n",
            "Epoch 8/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 1.4740\n",
            "Epoch 9/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 1.4083\n",
            "Epoch 10/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 1.3421\n",
            "Epoch 11/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 1.2768\n",
            "Epoch 12/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 1.2099\n",
            "Epoch 13/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 1.1376\n",
            "Epoch 14/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 1.0667\n",
            "Epoch 15/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.9929\n",
            "Epoch 16/60\n",
            "131/131 [==============================] - 7s 44ms/step - loss: 0.9187\n",
            "Epoch 17/60\n",
            "131/131 [==============================] - 6s 44ms/step - loss: 0.8457\n",
            "Epoch 18/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.7760\n",
            "Epoch 19/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.7104\n",
            "Epoch 20/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.6533\n",
            "Epoch 21/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.6036\n",
            "Epoch 22/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.5561\n",
            "Epoch 23/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 0.5146\n",
            "Epoch 24/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 0.4817\n",
            "Epoch 25/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.4537\n",
            "Epoch 26/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.4285\n",
            "Epoch 27/60\n",
            "131/131 [==============================] - 7s 44ms/step - loss: 0.4079\n",
            "Epoch 28/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3899\n",
            "Epoch 29/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3762\n",
            "Epoch 30/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3629\n",
            "Epoch 31/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3532\n",
            "Epoch 32/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3429\n",
            "Epoch 33/60\n",
            "131/131 [==============================] - 7s 44ms/step - loss: 0.3367\n",
            "Epoch 34/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3282\n",
            "Epoch 35/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3217\n",
            "Epoch 36/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3168\n",
            "Epoch 37/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3106\n",
            "Epoch 38/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.3068\n",
            "Epoch 39/60\n",
            "131/131 [==============================] - 7s 47ms/step - loss: 0.3007\n",
            "Epoch 40/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2977\n",
            "Epoch 41/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2971\n",
            "Epoch 42/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2929\n",
            "Epoch 43/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 0.2880\n",
            "Epoch 44/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 0.2860\n",
            "Epoch 45/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2852\n",
            "Epoch 46/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2809\n",
            "Epoch 47/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2777\n",
            "Epoch 48/60\n",
            "131/131 [==============================] - 6s 44ms/step - loss: 0.2735\n",
            "Epoch 49/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2742\n",
            "Epoch 50/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2708\n",
            "Epoch 51/60\n",
            "131/131 [==============================] - 7s 44ms/step - loss: 0.2707\n",
            "Epoch 52/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 0.2674\n",
            "Epoch 53/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2667\n",
            "Epoch 54/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 0.2653\n",
            "Epoch 55/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2651\n",
            "Epoch 56/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 0.2614\n",
            "Epoch 57/60\n",
            "131/131 [==============================] - 7s 46ms/step - loss: 0.2602\n",
            "Epoch 58/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2584\n",
            "Epoch 59/60\n",
            "131/131 [==============================] - 7s 45ms/step - loss: 0.2560\n",
            "Epoch 60/60\n",
            " 79/131 [=================>............] - ETA: 2s - loss: 0.2530"
          ]
        }
      ],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "lstm_sing = Sequential([\n",
        "    Embedding(n_vocab, 256,\n",
        "              batch_input_shape=[batch_size, None]),\n",
        "    LSTM(1024,\n",
        "         return_sequences=True,\n",
        "         stateful=True,\n",
        "         recurrent_initializer='glorot_uniform'),\n",
        "    Dense(n_vocab)\n",
        "])\n",
        "lstm_sing.summary()\n",
        "lstm_sing.compile(loss=loss, optimizer='adam')\n",
        "lstm_one = lstm_sing.fit(data, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vxQG0SSoQScj"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, num_generate, temperature=1):\n",
        "    input_eval = np.array([char2idx[s] for s in start_string])\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return start_string + ''.join(text_generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cmB5N1t0TICw",
        "outputId": "c7b2f265-1865-4975-b4cc-f001449c7422"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"привееееееееет, меня зовут костяhjSIШ§rb*xNp1xer[+JAe[iNx'ШЬXJХsiEЦl1EihXpШb§'[WoM\""
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_sing.build(tf.TensorShape([1, None]))\n",
        "generate_text(lstm_sing, 'привееееееееет, меня зовут костя', 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDf_OgMCmipa",
        "outputId": "66c9c396-41be-4e79-b571-ebeb53ae8ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (32, None, 256)           34816     \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (32, None, 1024)          5246976   \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (32, None, 1024)          8392704   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (32, None, 136)           139400    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,813,896\n",
            "Trainable params: 13,813,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "131/131 [==============================] - 24s 142ms/step - loss: 3.0646\n",
            "Epoch 2/20\n",
            "131/131 [==============================] - 16s 111ms/step - loss: 2.3730\n",
            "Epoch 3/20\n",
            "131/131 [==============================] - 15s 112ms/step - loss: 2.0140\n",
            "Epoch 4/20\n",
            "131/131 [==============================] - 15s 108ms/step - loss: 1.7762\n",
            "Epoch 5/20\n",
            "131/131 [==============================] - 15s 107ms/step - loss: 1.6253\n",
            "Epoch 6/20\n",
            "131/131 [==============================] - 15s 111ms/step - loss: 1.5098\n",
            "Epoch 7/20\n",
            "131/131 [==============================] - 16s 111ms/step - loss: 1.4095\n",
            "Epoch 8/20\n",
            "131/131 [==============================] - 15s 107ms/step - loss: 1.3140\n",
            "Epoch 9/20\n",
            "131/131 [==============================] - 15s 112ms/step - loss: 1.2161\n",
            "Epoch 10/20\n",
            "131/131 [==============================] - 17s 118ms/step - loss: 1.1121\n",
            "Epoch 11/20\n",
            "131/131 [==============================] - 16s 110ms/step - loss: 1.0057\n",
            "Epoch 12/20\n",
            "131/131 [==============================] - 15s 109ms/step - loss: 0.8967\n",
            "Epoch 13/20\n",
            "131/131 [==============================] - 15s 107ms/step - loss: 0.7903\n",
            "Epoch 14/20\n",
            "131/131 [==============================] - 15s 107ms/step - loss: 0.6898\n",
            "Epoch 15/20\n",
            "131/131 [==============================] - 15s 108ms/step - loss: 0.5977\n",
            "Epoch 16/20\n",
            "131/131 [==============================] - 15s 108ms/step - loss: 0.5195\n",
            "Epoch 17/20\n",
            "131/131 [==============================] - 15s 108ms/step - loss: 0.4570\n",
            "Epoch 18/20\n",
            "131/131 [==============================] - 15s 108ms/step - loss: 0.4078\n",
            "Epoch 19/20\n",
            "131/131 [==============================] - 15s 107ms/step - loss: 0.3698\n",
            "Epoch 20/20\n",
            "131/131 [==============================] - 15s 108ms/step - loss: 0.3430\n"
          ]
        }
      ],
      "source": [
        "lstm_double =  Sequential([\n",
        "                         Embedding(n_vocab, 256,\n",
        "                                   batch_input_shape=[batch_size, None]),\n",
        "                         LSTM(1024,\n",
        "                              return_sequences=True,\n",
        "                              stateful=True,\n",
        "                              recurrent_initializer='glorot_uniform'),\n",
        "                         LSTM(1024,\n",
        "                              return_sequences=True,\n",
        "                              stateful=True,\n",
        "                              recurrent_initializer='glorot_uniform'),\n",
        "                         Dense(n_vocab)\n",
        "                         ])\n",
        "lstm_double.summary()\n",
        "\n",
        "\n",
        "\n",
        "lstm_double.compile(loss= loss, optimizer='adam')\n",
        "\n",
        "\n",
        "lstm_double_dir = './lstm2_checkpoints'\n",
        "lstm_double_prefix = os.path.join(lstm_double_dir, \"checkpoint_{epoch}\")\n",
        "lstm_double_checkpoint = ModelCheckpoint(filepath=lstm_double_prefix, save_weights_only=True)\n",
        "\n",
        "\n",
        "lstm_two = lstm_double.fit(data, epochs=num_epochs,callbacks=[lstm_double_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zYmjddrBnWJG",
        "outputId": "7ef3ce3f-a36c-4516-d9d3-907e87a45bc3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'привет, меня зовут костянуть с учительница у меня и Эль\\r\\nразрие есть  кото'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_double =  Sequential([\n",
        "                         Embedding(n_vocab, 256,\n",
        "                                   batch_input_shape=[1, None]),\n",
        "                         LSTM(1024,\n",
        "                              return_sequences=True,\n",
        "                              stateful=True,\n",
        "                              recurrent_initializer='glorot_uniform'),\n",
        "                         LSTM(1024,\n",
        "                              return_sequences=True,\n",
        "                              stateful=True,\n",
        "                              recurrent_initializer='glorot_uniform'),\n",
        "                         Dense(n_vocab)\n",
        "                         ])\n",
        "\n",
        "\n",
        "\n",
        "lstm_double.load_weights(tf.train.latest_checkpoint(lstm_double_dir))\n",
        "\n",
        "\n",
        "\n",
        "lstm_double.build(tf.TensorShape([1, None]))\n",
        "generate_text(lstm_double, 'привет, меня зовут костя', 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcgrKhwtnB56",
        "outputId": "d554fed2-33c0-405c-c967-2013a3b167e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_11 (Embedding)    (32, None, 256)           34816     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (32, None, 1024)          1311744   \n",
            "                                                                 \n",
            " dense_11 (Dense)            (32, None, 136)           139400    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,485,960\n",
            "Trainable params: 1,485,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "131/131 [==============================] - 17s 115ms/step - loss: 2.9185\n",
            "Epoch 2/20\n",
            "131/131 [==============================] - 13s 91ms/step - loss: 2.3928\n",
            "Epoch 3/20\n",
            "131/131 [==============================] - 15s 107ms/step - loss: 2.1951\n",
            "Epoch 4/20\n",
            "131/131 [==============================] - 13s 93ms/step - loss: 2.0432\n",
            "Epoch 5/20\n",
            "131/131 [==============================] - 13s 92ms/step - loss: 1.9278\n",
            "Epoch 6/20\n",
            "131/131 [==============================] - 12s 85ms/step - loss: 1.8342\n",
            "Epoch 7/20\n",
            "131/131 [==============================] - 12s 89ms/step - loss: 1.7637\n",
            "Epoch 8/20\n",
            "131/131 [==============================] - 13s 89ms/step - loss: 1.7094\n",
            "Epoch 9/20\n",
            "131/131 [==============================] - 13s 90ms/step - loss: 1.6604\n",
            "Epoch 10/20\n",
            "131/131 [==============================] - 12s 90ms/step - loss: 1.6187\n",
            "Epoch 11/20\n",
            "131/131 [==============================] - 12s 90ms/step - loss: 1.5810\n",
            "Epoch 12/20\n",
            "131/131 [==============================] - 12s 83ms/step - loss: 1.5503\n",
            "Epoch 13/20\n",
            "131/131 [==============================] - 12s 90ms/step - loss: 1.5216\n",
            "Epoch 14/20\n",
            "131/131 [==============================] - 12s 89ms/step - loss: 1.4899\n",
            "Epoch 15/20\n",
            "131/131 [==============================] - 12s 81ms/step - loss: 1.4669\n",
            "Epoch 16/20\n",
            "131/131 [==============================] - 15s 103ms/step - loss: 1.4409\n",
            "Epoch 17/20\n",
            "131/131 [==============================] - 11s 77ms/step - loss: 1.4150\n",
            "Epoch 18/20\n",
            "131/131 [==============================] - 12s 88ms/step - loss: 1.3931\n",
            "Epoch 19/20\n",
            "131/131 [==============================] - 13s 92ms/step - loss: 1.3689\n",
            "Epoch 20/20\n",
            "131/131 [==============================] - 13s 92ms/step - loss: 1.3450\n"
          ]
        }
      ],
      "source": [
        "rnn = Sequential([\n",
        "                  Embedding(n_vocab, 256,\n",
        "                            batch_input_shape=[batch_size, None]),\n",
        "                  SimpleRNN(1024,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "                  Dense(n_vocab)\n",
        "                  ])\n",
        "\n",
        "rnn.summary()\n",
        "\n",
        "\n",
        "\n",
        "rnn.compile(loss= loss, optimizer='adam')\n",
        "\n",
        "\n",
        "rnn_dir = './rnn_checkpoints'\n",
        "rnn_prefix = os.path.join(rnn_dir, \"checkpoint_{epoch}\")\n",
        "rnn_checkpoint = ModelCheckpoint(filepath=rnn_prefix, save_weights_only=True)\n",
        "\n",
        "\n",
        "RNN = rnn.fit(data, epochs=num_epochs,callbacks=[rnn_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oOtiMjrxnkHH",
        "outputId": "6caee27b-6c07-4698-e250-96800e3d2b61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'привет, меня зовут костяM\\nLETSЬ*******yk/\\nGiGЪЬgЪkЬ******ncw********kSb***'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "rnn = Sequential([\n",
        "                  Embedding(n_vocab, 256,\n",
        "                            batch_input_shape=[1, None]),\n",
        "                  SimpleRNN(1024,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "                  Dense(n_vocab)\n",
        "                  ])\n",
        "\n",
        "\n",
        "\n",
        "rnn.load_weights(tf.train.latest_checkpoint(rnn_dir))\n",
        "\n",
        "\n",
        "\n",
        "rnn.build(tf.TensorShape([1, None]))\n",
        "generate_text(rnn, 'привет, меня зовут костя', 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
